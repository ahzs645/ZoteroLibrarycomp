{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Portal library...\n",
      "\n",
      "Processing RDF: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Nechako Portal.rdf\n",
      "\n",
      "Processing deduplicated XML: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml\n",
      "\n",
      "Processing Search library...\n",
      "\n",
      "Processing RDF: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Nechako Saturation Search (2024-04).rdf\n",
      "\n",
      "Processing deduplicated XML: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml\n",
      "\n",
      "Generating integrated analysis...\n",
      "Error in main: 'ZoteroIntegrator' object has no attribute 'create_analysis'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_94336/541069924.py\", line 238, in main\n",
      "    integrator.create_analysis(portal_data, search_data, output_dir)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'ZoteroIntegrator' object has no attribute 'create_analysis'\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import traceback\n",
    "\n",
    "class ZoteroIntegrator:\n",
    "    def __init__(self):\n",
    "        self.namespaces = {\n",
    "            'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "            'z': 'http://www.zotero.org/namespaces/export#',\n",
    "            'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "            'bib': 'http://purl.org/net/biblio#',\n",
    "            'dcterms': 'http://purl.org/dc/terms/',\n",
    "            'foaf': 'http://xmlns.com/foaf/0.1/'\n",
    "        }\n",
    "\n",
    "    def normalize_title(self, title):\n",
    "        \"\"\"Normalize title for comparison.\"\"\"\n",
    "        if not title:\n",
    "            return \"\"\n",
    "        title = re.sub(r'[^\\w\\s]', '', title.lower())\n",
    "        return ' '.join(title.split())\n",
    "\n",
    "    def extract_from_rdf(self, rdf_path):\n",
    "        \"\"\"Extract collection structure and items from RDF.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nProcessing RDF: {rdf_path}\")\n",
    "            tree = ET.parse(rdf_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            print(\"Extracting collections...\")\n",
    "            collections = {}\n",
    "            collection_items = defaultdict(set)\n",
    "            \n",
    "            for collection in root.findall(f\".//{{{self.namespaces['z']}}}Collection\"):\n",
    "                coll_id = collection.get(f\"{{{self.namespaces['rdf']}}}about\")\n",
    "                if coll_id:\n",
    "                    title = collection.find(f\".//{{{self.namespaces['dc']}}}title\")\n",
    "                    if title is not None and title.text:\n",
    "                        collections[coll_id] = {\n",
    "                            'title': title.text,\n",
    "                            'items': set(),\n",
    "                            'parent': None\n",
    "                        }\n",
    "                        \n",
    "                        # Get items in this collection\n",
    "                        for item_ref in collection.findall(f\".//{{{self.namespaces['dcterms']}}}hasPart\"):\n",
    "                            item_id = item_ref.get(f\"{{{self.namespaces['rdf']}}}resource\")\n",
    "                            if item_id:\n",
    "                                collections[coll_id]['items'].add(item_id)\n",
    "                                collection_items[title.text].add(item_id)\n",
    "            \n",
    "            print(f\"Found {len(collections)} collections\")\n",
    "            \n",
    "            print(\"Extracting items...\")\n",
    "            items = {}\n",
    "            for item in root.findall(\".//*[@rdf:about]\", self.namespaces):\n",
    "                item_id = item.get(f\"{{{self.namespaces['rdf']}}}about\")\n",
    "                if item_id:\n",
    "                    title = item.find(f\".//{{{self.namespaces['dc']}}}title\")\n",
    "                    if title is not None and title.text:\n",
    "                        items[item_id] = {\n",
    "                            'title': title.text,\n",
    "                            'normalized_title': self.normalize_title(title.text),\n",
    "                            'collections': set()\n",
    "                        }\n",
    "            \n",
    "            print(f\"Found {len(items)} items\")\n",
    "            return collections, items, collection_items\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_from_rdf: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return {}, {}, defaultdict(set)\n",
    "\n",
    "    def extract_from_dedup_xml(self, xml_path):\n",
    "        try:\n",
    "            print(f\"\\nProcessing deduplicated XML: {xml_path}\")\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            records = []\n",
    "            for record in root.findall('.//record'):\n",
    "                try:\n",
    "                    rec_data = {}\n",
    "                    \n",
    "                    # Extract title with style handling\n",
    "                    titles = record.find('.//titles')\n",
    "                    if titles is not None:\n",
    "                        # Primary title\n",
    "                        title = titles.find('title')\n",
    "                        if title is not None:\n",
    "                            style = title.find('.//style')\n",
    "                            if style is not None and style.text:\n",
    "                                rec_data['title'] = style.text.strip()\n",
    "                                rec_data['normalized_title'] = self.normalize_title(style.text)\n",
    "                        \n",
    "                        # Secondary title (journal)\n",
    "                        secondary = titles.find('secondary-title')\n",
    "                        if secondary is not None:\n",
    "                            style = secondary.find('.//style')\n",
    "                            if style is not None and style.text:\n",
    "                                rec_data['journal'] = style.text.strip()\n",
    "                    \n",
    "                    # Extract authors\n",
    "                    authors = []\n",
    "                    contributors = record.find('.//contributors')\n",
    "                    if contributors is not None:\n",
    "                        for author in contributors.findall('.//author'):\n",
    "                            style = author.find('.//style')\n",
    "                            if style is not None and style.text:\n",
    "                                authors.append(style.text.strip())\n",
    "                    rec_data['authors'] = authors\n",
    "                    \n",
    "                    # Extract abstract\n",
    "                    abstract = record.find('.//abstract')\n",
    "                    if abstract is not None:\n",
    "                        style = abstract.find('.//style')\n",
    "                        if style is not None and style.text:\n",
    "                            rec_data['abstract'] = style.text.strip()\n",
    "                    \n",
    "                    # Extract DOI/URL\n",
    "                    electronic_num = record.find('.//electronic-resource-num')\n",
    "                    if electronic_num is not None:\n",
    "                        style = electronic_num.find('.//style')\n",
    "                        if style is not None and style.text:\n",
    "                            rec_data['doi'] = style.text.strip()\n",
    "                    \n",
    "                    # Extract type\n",
    "                    ref_type = record.find('.//ref-type')\n",
    "                    if ref_type is not None:\n",
    "                        rec_data['type'] = ref_type.get('name')\n",
    "                    \n",
    "                    # Only add records with titles\n",
    "                    if 'title' in rec_data:\n",
    "                        records.append(rec_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing record: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Extracted {len(records)} records from XML\")\n",
    "            return records\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_from_dedup_xml: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "    def visualize_hierarchy(self, collections, title, output_path):\n",
    "        \"\"\"Create visualization of collection hierarchy.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nCreating hierarchy visualization: {title}\")\n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Add nodes and edges\n",
    "            for coll_id, coll in collections.items():\n",
    "                G.add_node(coll['title'])\n",
    "                if coll.get('parent') and coll['parent'] in collections:\n",
    "                    parent_title = collections[coll['parent']]['title']\n",
    "                    G.add_edge(parent_title, coll['title'])\n",
    "            \n",
    "            plt.figure(figsize=(15, 10))\n",
    "            pos = nx.spring_layout(G)\n",
    "            nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "                    node_size=2000, font_size=8, font_weight='bold',\n",
    "                    arrows=True)\n",
    "            plt.title(title)\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved visualization to: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in visualize_hierarchy: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def create_analysis(self, portal_data, search_data, output_dir):\n",
    "        \"\"\"Create comprehensive analysis combining RDF and XML data.\"\"\"\n",
    "        try:\n",
    "            print(\"\\nCreating integrated analysis...\")\n",
    "            collections_p, items_p, coll_items_p = portal_data['rdf']\n",
    "            collections_s, items_s, coll_items_s = search_data['rdf']\n",
    "            records_p = portal_data['xml']\n",
    "            records_s = search_data['xml']\n",
    "            \n",
    "            # Create output directories\n",
    "            vis_dir = os.path.join(output_dir, 'visualizations')\n",
    "            os.makedirs(vis_dir, exist_ok=True)\n",
    "            \n",
    "            print(\"Creating visualizations...\")\n",
    "            self.visualize_hierarchy(collections_p, 'Portal Collections', \n",
    "                                   os.path.join(vis_dir, 'portal_hierarchy.png'))\n",
    "            self.visualize_hierarchy(collections_s, 'Search Collections',\n",
    "                                   os.path.join(vis_dir, 'search_hierarchy.png'))\n",
    "            \n",
    "            print(\"Creating Excel report...\")\n",
    "            excel_path = os.path.join(output_dir, 'integrated_analysis.xlsx')\n",
    "            with pd.ExcelWriter(excel_path) as writer:\n",
    "                # Collection statistics\n",
    "                print(\"- Creating collection statistics...\")\n",
    "                coll_stats = []\n",
    "                for coll_title, items in coll_items_p.items():\n",
    "                    coll_stats.append({\n",
    "                        'Collection': coll_title,\n",
    "                        'Library': 'Portal',\n",
    "                        'Total Items': len(items),\n",
    "                        'Unique Items': len({items_p[i]['normalized_title'] for i in items if i in items_p})\n",
    "                    })\n",
    "                for coll_title, items in coll_items_s.items():\n",
    "                    coll_stats.append({\n",
    "                        'Collection': coll_title,\n",
    "                        'Library': 'Search',\n",
    "                        'Total Items': len(items),\n",
    "                        'Unique Items': len({items_s[i]['normalized_title'] for i in items if i in items_s})\n",
    "                    })\n",
    "                pd.DataFrame(coll_stats).to_excel(writer, sheet_name='Collection Stats', index=False)\n",
    "                \n",
    "                # Duplicate analysis\n",
    "                print(\"- Analyzing duplicates...\")\n",
    "                portal_titles = {r['normalized_title'] for r in records_p}\n",
    "                search_titles = {r['normalized_title'] for r in records_s}\n",
    "                duplicates = portal_titles.intersection(search_titles)\n",
    "                \n",
    "                dupes_data = []\n",
    "                for title in duplicates:\n",
    "                    portal_colls = [coll for coll, items in coll_items_p.items() \n",
    "                                  if any(items_p.get(i, {}).get('normalized_title') == title for i in items)]\n",
    "                    search_colls = [coll for coll, items in coll_items_s.items()\n",
    "                                  if any(items_s.get(i, {}).get('normalized_title') == title for i in items)]\n",
    "                    dupes_data.append({\n",
    "                        'Title': title,\n",
    "                        'Portal Collections': '; '.join(portal_colls),\n",
    "                        'Search Collections': '; '.join(search_colls)\n",
    "                    })\n",
    "                pd.DataFrame(dupes_data).to_excel(writer, sheet_name='Duplicates', index=False)\n",
    "                \n",
    "                # Summary\n",
    "                print(\"- Creating summary...\")\n",
    "                summary = pd.DataFrame([\n",
    "                    {'Metric': 'Total Portal Items', 'Value': len(records_p)},\n",
    "                    {'Metric': 'Total Search Items', 'Value': len(records_s)},\n",
    "                    {'Metric': 'Duplicate Items', 'Value': len(duplicates)},\n",
    "                    {'Metric': 'Portal Collections', 'Value': len(collections_p)},\n",
    "                    {'Metric': 'Search Collections', 'Value': len(collections_s)}\n",
    "                ])\n",
    "                summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            print(f\"Analysis saved to: {excel_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in create_analysis: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        base_dir = '/Users/ahmadjalil/Desktop/Zotero Project'\n",
    "        files = {\n",
    "            'portal': {\n",
    "                'rdf': os.path.join(base_dir, 'Nechako Portal/Nechako Portal.rdf'),\n",
    "                'xml': os.path.join(base_dir, 'Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml')\n",
    "            },\n",
    "            'search': {\n",
    "                'rdf': os.path.join(base_dir, 'Nechako Saturation Search/Nechako Saturation Search (2024-04).rdf'),\n",
    "                'xml': os.path.join(base_dir, 'Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        integrator = ZoteroIntegrator()\n",
    "        \n",
    "        # Process Portal library\n",
    "        print(\"\\nProcessing Portal library...\")\n",
    "        portal_data = {\n",
    "            'rdf': integrator.extract_from_rdf(files['portal']['rdf']),\n",
    "            'xml': integrator.extract_from_dedup_xml(files['portal']['xml'])\n",
    "        }\n",
    "        \n",
    "        # Process Search library\n",
    "        print(\"\\nProcessing Search library...\")\n",
    "        search_data = {\n",
    "            'rdf': integrator.extract_from_rdf(files['search']['rdf']),\n",
    "            'xml': integrator.extract_from_dedup_xml(files['search']['xml'])\n",
    "        }\n",
    "        \n",
    "        # Generate analysis\n",
    "        output_dir = os.path.join(base_dir, 'Integrated_Results')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nGenerating integrated analysis...\")\n",
    "        integrator.create_analysis(portal_data, search_data, output_dir)\n",
    "        \n",
    "        print(\"\\nAnalysis complete! Results saved to:\", output_dir)\n",
    "        print(\"\\nCreated:\")\n",
    "        print(\"1. Collection hierarchy visualizations\")\n",
    "        print(\"2. Integrated analysis Excel file with:\")\n",
    "        print(\"   - Collection statistics\")\n",
    "        print(\"   - Duplicate analysis\")\n",
    "        print(\"   - Summary metrics\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (3.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'portal_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 201\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# First convert the data\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m df_items, df_collections \u001b[38;5;241m=\u001b[39m convert_to_dataframes(portal_data, search_data)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Then run any of the analysis functions\u001b[39;00m\n\u001b[1;32m    204\u001b[0m analyze_duplicate_concentrations(df_items, df_collections)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'portal_data' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install plotly seaborn networkx\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def convert_to_dataframes(portal_data, search_data):\n",
    "    \"\"\"Convert the extracted data into pandas DataFrames for analysis.\"\"\"\n",
    "    # Unpack the data\n",
    "    collections_p, items_p, coll_items_p = portal_data['rdf']\n",
    "    collections_s, items_s, coll_items_s = search_data['rdf']\n",
    "    records_p = portal_data['xml']\n",
    "    records_s = search_data['xml']\n",
    "    \n",
    "    # Create items DataFrame\n",
    "    items_data = []\n",
    "    \n",
    "    # Add Portal items\n",
    "    for record in records_p:\n",
    "        for coll_title, items in coll_items_p.items():\n",
    "            # Check if this record's title matches any item in this collection\n",
    "            if any(record['normalized_title'] == items_p.get(item_id, {}).get('normalized_title') \n",
    "                   for item_id in items if item_id in items_p):\n",
    "                items_data.append({\n",
    "                    'Title': record.get('title', ''),\n",
    "                    'normalized_title': record.get('normalized_title', ''),\n",
    "                    'Collections': coll_title,\n",
    "                    'library': 'Portal',\n",
    "                    'type': record.get('type', ''),\n",
    "                    'authors': record.get('authors', []),\n",
    "                    'doi': record.get('doi', ''),\n",
    "                    'journal': record.get('journal', '')\n",
    "                })\n",
    "    \n",
    "    # Add Search items\n",
    "    for record in records_s:\n",
    "        for coll_title, items in coll_items_s.items():\n",
    "            if any(record['normalized_title'] == items_s.get(item_id, {}).get('normalized_title')\n",
    "                   for item_id in items if item_id in items_s):\n",
    "                items_data.append({\n",
    "                    'Title': record.get('title', ''),\n",
    "                    'normalized_title': record.get('normalized_title', ''),\n",
    "                    'Collections': coll_title,\n",
    "                    'library': 'Search',\n",
    "                    'type': record.get('type', ''),\n",
    "                    'authors': record.get('authors', []),\n",
    "                    'doi': record.get('doi', ''),\n",
    "                    'journal': record.get('journal', '')\n",
    "                })\n",
    "    \n",
    "    df_items = pd.DataFrame(items_data)\n",
    "    \n",
    "    # Create collections DataFrame\n",
    "    collections_data = []\n",
    "    for coll_title, items in coll_items_p.items():\n",
    "        collections_data.append({\n",
    "            'Collection': coll_title,\n",
    "            'Library': 'Portal',\n",
    "            'Items': len(items)\n",
    "        })\n",
    "    for coll_title, items in coll_items_s.items():\n",
    "        collections_data.append({\n",
    "            'Collection': coll_title,\n",
    "            'Library': 'Search',\n",
    "            'Items': len(items)\n",
    "        })\n",
    "    \n",
    "    df_collections = pd.DataFrame(collections_data)\n",
    "    \n",
    "    return df_items, df_collections\n",
    "\n",
    "def analyze_duplicate_concentrations(df_items, df_collections):\n",
    "    \"\"\"Analyze where duplicates are concentrated.\"\"\"\n",
    "    print(\"\\nAnalyzing duplicate concentrations...\")\n",
    "    \n",
    "    # Find duplicates\n",
    "    dupes = df_items[df_items.duplicated(subset='normalized_title', keep=False)]\n",
    "    \n",
    "    # Count duplicates by collection\n",
    "    dupe_counts = dupes.groupby('Collections')['Title'].count().sort_values(ascending=False)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    dupe_counts.plot(kind='bar')\n",
    "    plt.title('Duplicate Concentration by Collection')\n",
    "    plt.xlabel('Collection')\n",
    "    plt.ylabel('Number of Duplicates')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDuplicate Statistics by Collection:\")\n",
    "    total_items = df_items.groupby('Collections')['Title'].count()\n",
    "    for coll in dupe_counts.index:\n",
    "        print(f\"\\n{coll}:\")\n",
    "        print(f\"Total items: {total_items[coll]}\")\n",
    "        print(f\"Duplicates: {dupe_counts[coll]}\")\n",
    "        print(f\"Duplicate percentage: {(dupe_counts[coll]/total_items[coll]*100):.1f}%\")\n",
    "\n",
    "def visualize_collection_relationships(df_items):\n",
    "    \"\"\"Create visualization showing relationships between collections.\"\"\"\n",
    "    # Create a matrix of collection overlaps\n",
    "    collections = df_items['Collections'].unique()\n",
    "    overlap_matrix = pd.DataFrame(0, index=collections, columns=collections)\n",
    "    \n",
    "    # Calculate overlaps\n",
    "    for title in df_items[df_items.duplicated(subset='normalized_title', keep=False)]['normalized_title'].unique():\n",
    "        colls = df_items[df_items['normalized_title'] == title]['Collections'].unique()\n",
    "        for c1 in colls:\n",
    "            for c2 in colls:\n",
    "                if c1 != c2:\n",
    "                    overlap_matrix.loc[c1, c2] += 1\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(overlap_matrix, annot=True, fmt='g', cmap='YlOrRd')\n",
    "    plt.title('Collection Overlap Heatmap')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_document_types(df_items):\n",
    "    \"\"\"Analyze which types of documents tend to overlap.\"\"\"\n",
    "    # Count document types in duplicates\n",
    "    dupes = df_items[df_items.duplicated(subset='normalized_title', keep=False)]\n",
    "    type_counts = dupes.groupby(['type', 'library'])['Title'].count().unstack(fill_value=0)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    type_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Document Types in Duplicates')\n",
    "    plt.xlabel('Document Type')\n",
    "    plt.ylabel('Number of Items')\n",
    "    plt.legend(title='Library')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDocument Type Statistics in Duplicates:\")\n",
    "    for doc_type in type_counts.index:\n",
    "        print(f\"\\n{doc_type}:\")\n",
    "        for library in type_counts.columns:\n",
    "            print(f\"{library}: {type_counts.loc[doc_type, library]} items\")\n",
    "\n",
    "\n",
    "def analyze_categorization_patterns(df_items):\n",
    "    \"\"\"Analyze how items are categorized differently between libraries.\"\"\"\n",
    "    # Find items that appear in both libraries\n",
    "    dupes = df_items[df_items.duplicated(subset='normalized_title', keep=False)]\n",
    "    \n",
    "    # Create a mapping of categorization patterns\n",
    "    categorization_patterns = defaultdict(int)\n",
    "    for title in dupes['normalized_title'].unique():\n",
    "        items = dupes[dupes['normalized_title'] == title]\n",
    "        portal_cats = items[items['library'] == 'Portal']['Collections'].iloc[0]\n",
    "        search_cats = items[items['library'] == 'Search']['Collections'].iloc[0]\n",
    "        categorization_patterns[(portal_cats, search_cats)] += 1\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    patterns_df = pd.DataFrame(\n",
    "        [(p[0], p[1], c) for p, c in categorization_patterns.items()],\n",
    "        columns=['Portal Category', 'Search Category', 'Count']\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(\n",
    "        patterns_df.pivot(\n",
    "            index='Portal Category',\n",
    "            columns='Search Category',\n",
    "            values='Count'\n",
    "        ),\n",
    "        annot=True,\n",
    "        fmt='g',\n",
    "        cmap='YlOrRd'\n",
    "    )\n",
    "    plt.title('Categorization Patterns Between Libraries')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interesting patterns\n",
    "    print(\"\\nNotable Categorization Patterns:\")\n",
    "    for (portal_cat, search_cat), count in sorted(\n",
    "        categorization_patterns.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]:\n",
    "        print(f\"\\nItems categorized as:\")\n",
    "        print(f\"Portal: {portal_cat}\")\n",
    "        print(f\"Search: {search_cat}\")\n",
    "        print(f\"Count: {count} items\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df_items and df_collections are your existing DataFrames\n",
    "analyze_duplicate_concentrations(df_items, df_collections)\n",
    "visualize_collection_flow(df_items)\n",
    "analyze_document_overlap(df_items)\n",
    "analyze_categorization_patterns(df_items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
