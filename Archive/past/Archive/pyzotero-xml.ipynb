{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 12:28:27 - Processing portal library...\n",
      "Fetching portal items:  54%|█████▍    | 2600/4789 [00:14<00:12, 180.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m     fetch_initial_data()\n",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m, in \u001b[0;36mfetch_initial_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(items))\n\u001b[1;32m     67\u001b[0m         start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m---> 68\u001b[0m         sleep(\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# Prevent overloading\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame and save\u001b[39;00m\n\u001b[1;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_items)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyzotero import zotero\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging configuration\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def fetch_initial_data():\n",
    "    \"\"\"Fetch data from Zotero API and save to CSV files\"\"\"\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = 'zotero_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize Zotero connections\n",
    "    libraries = {\n",
    "        'portal': zotero.Zotero('364018', 'group', local=True),\n",
    "        'search': zotero.Zotero('5494504', 'group', local=True)\n",
    "    }\n",
    "    \n",
    "    for lib_name, zot in libraries.items():\n",
    "        logger.info(f\"Processing {lib_name} library...\")\n",
    "        \n",
    "        try:\n",
    "            # Get total count\n",
    "            total = zot.count_items()\n",
    "            \n",
    "            # Fetch all items\n",
    "            all_items = []\n",
    "            start = 0\n",
    "            limit = 50\n",
    "            \n",
    "            with tqdm(total=total, desc=f\"Fetching {lib_name} items\") as pbar:\n",
    "                while start < total:\n",
    "                    items = zot.items(start=start, limit=limit)\n",
    "                    if not items:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract relevant fields from each item\n",
    "                    for item in items:\n",
    "                        data = item['data']\n",
    "                        processed_item = {\n",
    "                            'key': item['key'],\n",
    "                            'title': data.get('title', ''),\n",
    "                            'itemType': data.get('itemType', ''),\n",
    "                            'creators': str(data.get('creators', [])),  # Convert list to string\n",
    "                            'date': data.get('date', ''),\n",
    "                            'DOI': data.get('DOI', ''),\n",
    "                            'url': data.get('url', ''),\n",
    "                            'collections': str(data.get('collections', [])),  # Convert list to string\n",
    "                            'tags': str(data.get('tags', [])),  # Convert list to string\n",
    "                            'abstractNote': data.get('abstractNote', '')\n",
    "                        }\n",
    "                        all_items.append(processed_item)\n",
    "                    \n",
    "                    pbar.update(len(items))\n",
    "                    start += limit\n",
    "                    sleep(0.5)  # Prevent overloading\n",
    "            \n",
    "            # Convert to DataFrame and save\n",
    "            df = pd.DataFrame(all_items)\n",
    "            csv_path = os.path.join(output_dir, f'{lib_name}_library.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Saved {lib_name} library to {csv_path}\")\n",
    "            \n",
    "            # Also save as pickle for preserving data types\n",
    "            pickle_path = os.path.join(output_dir, f'{lib_name}_library.pkl')\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Saved {lib_name} library to {pickle_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {lib_name} library: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_initial_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Analysis:\n",
      "Portal Library Items: 100\n",
      "Search Library Items: 71\n",
      "Duplicate Items: 1\n",
      "\n",
      "Collection Statistics:\n",
      "\n",
      "Portal Library Collections:\n",
      "collections\n",
      "WTI7SWR5    8352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Search Library Collections:\n",
      "collections\n",
      "MRW66E3Y    1134\n",
      "4EFHUEJI     702\n",
      "5JRIR9ZQ      54\n",
      "EG5BYG2X      27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import ast\n",
    "import os\n",
    "\n",
    "class ZoteroAnalyzer:\n",
    "    def __init__(self, data_dir='zotero_data'):\n",
    "        \"\"\"Initialize analyzer with saved data\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.libraries = {}\n",
    "        \n",
    "        # Load saved data\n",
    "        for lib_name in ['portal', 'search']:\n",
    "            pickle_path = os.path.join(data_dir, f'{lib_name}_library.pkl')\n",
    "            if os.path.exists(pickle_path):\n",
    "                self.libraries[lib_name] = pd.read_pickle(pickle_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No saved data found for {lib_name} library\")\n",
    "    \n",
    "    def analyze_duplicates(self):\n",
    "        \"\"\"Find duplicates between libraries\"\"\"\n",
    "        # Get sets of titles from each library\n",
    "        portal_titles = set(self.libraries['portal']['title'].str.lower())\n",
    "        search_titles = set(self.libraries['search']['title'].str.lower())\n",
    "        \n",
    "        # Find duplicates\n",
    "        duplicate_titles = portal_titles & search_titles\n",
    "        \n",
    "        # Create detailed duplicates DataFrame\n",
    "        duplicates_data = []\n",
    "        for title in duplicate_titles:\n",
    "            portal_item = self.libraries['portal'][self.libraries['portal']['title'].str.lower() == title].iloc[0]\n",
    "            search_item = self.libraries['search'][self.libraries['search']['title'].str.lower() == title].iloc[0]\n",
    "            \n",
    "            duplicates_data.append({\n",
    "                'title': title,\n",
    "                'portal_key': portal_item['key'],\n",
    "                'search_key': search_item['key'],\n",
    "                'item_type': portal_item['itemType']\n",
    "            })\n",
    "        \n",
    "        duplicates_df = pd.DataFrame(duplicates_data)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        venn2([portal_titles, search_titles],\n",
    "              set_labels=('Portal Library', 'Search Library'))\n",
    "        plt.title('Library Content Overlap')\n",
    "        plt.savefig('library_overlap.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return {\n",
    "            'duplicates': duplicate_titles,\n",
    "            'duplicates_df': duplicates_df,\n",
    "            'stats': {\n",
    "                'portal_total': len(portal_titles),\n",
    "                'search_total': len(search_titles),\n",
    "                'duplicate_count': len(duplicate_titles)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_collections(self):\n",
    "        \"\"\"Analyze collection distribution\"\"\"\n",
    "        collection_stats = {}\n",
    "        \n",
    "        for lib_name, df in self.libraries.items():\n",
    "            # Convert string representation of collections back to list\n",
    "            df['collections'] = df['collections'].apply(ast.literal_eval)\n",
    "            \n",
    "            # Count items per collection\n",
    "            collection_counts = df.explode('collections')['collections'].value_counts()\n",
    "            collection_stats[lib_name] = collection_counts\n",
    "        \n",
    "        return collection_stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ZoteroAnalyzer()\n",
    "    \n",
    "    # Analyze duplicates\n",
    "    duplicate_analysis = analyzer.analyze_duplicates()\n",
    "    print(\"\\nDuplicate Analysis:\")\n",
    "    print(f\"Portal Library Items: {duplicate_analysis['stats']['portal_total']}\")\n",
    "    print(f\"Search Library Items: {duplicate_analysis['stats']['search_total']}\")\n",
    "    print(f\"Duplicate Items: {duplicate_analysis['stats']['duplicate_count']}\")\n",
    "    \n",
    "    # Analyze collections\n",
    "    collection_stats = analyzer.analyze_collections()\n",
    "    print(\"\\nCollection Statistics:\")\n",
    "    for lib_name, stats in collection_stats.items():\n",
    "        print(f\"\\n{lib_name.title()} Library Collections:\")\n",
    "        print(stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class XMLProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the XML processor\"\"\"\n",
    "        self.xml_data = {\n",
    "            'portal': {\n",
    "                'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml',\n",
    "                'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml'\n",
    "            },\n",
    "            'search': {\n",
    "                'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml',\n",
    "                'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def parse_xml_file(self, file_path):\n",
    "        \"\"\"Parse XML file and extract relevant information\"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            items = []\n",
    "            for entry in root.findall('.//entry'):\n",
    "                item = {}\n",
    "                \n",
    "                # Extract basic metadata\n",
    "                item['title'] = entry.findtext('title', '')\n",
    "                item['id'] = entry.findtext('id', '')\n",
    "                item['updated'] = entry.findtext('updated', '')\n",
    "                \n",
    "                # Extract authors\n",
    "                authors = []\n",
    "                for author in entry.findall('.//author'):\n",
    "                    authors.append(author.findtext('name', ''))\n",
    "                item['authors'] = authors\n",
    "                \n",
    "                # Extract DOI if available\n",
    "                doi = entry.find(\".//zapi:DOI\", namespaces={'zapi': 'http://zotero.org/ns/api'})\n",
    "                item['DOI'] = doi.text if doi is not None else ''\n",
    "                \n",
    "                items.append(item)\n",
    "            \n",
    "            return pd.DataFrame(items)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def process_all_files(self):\n",
    "        \"\"\"Process all XML files and return consolidated data\"\"\"\n",
    "        results = {}\n",
    "        for lib_name, paths in self.xml_data.items():\n",
    "            results[lib_name] = {\n",
    "                'deduped': self.parse_xml_file(paths['deduped']),\n",
    "                'duplicates': self.parse_xml_file(paths['duplicates'])\n",
    "            }\n",
    "        return results\n",
    "\n",
    "    def match_with_zotero(self, xml_data, zotero_df):\n",
    "        \"\"\"Match XML entries with Zotero library entries\"\"\"\n",
    "        matched_data = []\n",
    "        \n",
    "        for _, xml_row in xml_data.iterrows():\n",
    "            # Try matching by DOI first\n",
    "            if xml_row['DOI']:\n",
    "                zotero_match = zotero_df[zotero_df['DOI'] == xml_row['DOI']]\n",
    "                if not zotero_match.empty:\n",
    "                    matched_data.append({\n",
    "                        'xml_title': xml_row['title'],\n",
    "                        'zotero_title': zotero_match.iloc[0]['title'],\n",
    "                        'match_method': 'DOI',\n",
    "                        'xml_id': xml_row['id'],\n",
    "                        'zotero_key': zotero_match.iloc[0]['key']\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Try matching by title (case-insensitive)\n",
    "            zotero_match = zotero_df[zotero_df['title'].str.lower() == xml_row['title'].lower()]\n",
    "            if not zotero_match.empty:\n",
    "                matched_data.append({\n",
    "                    'xml_title': xml_row['title'],\n",
    "                    'zotero_title': zotero_match.iloc[0]['title'],\n",
    "                    'match_method': 'title',\n",
    "                    'xml_id': xml_row['id'],\n",
    "                    'zotero_key': zotero_match.iloc[0]['key']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(matched_data)\n",
    "\n",
    "def analyze_xml_and_zotero():\n",
    "    \"\"\"Main function to analyze XML files and match with Zotero data\"\"\"\n",
    "    # Initialize processors\n",
    "    xml_processor = XMLProcessor()\n",
    "    zotero_analyzer = ZoteroAnalyzer()\n",
    "    \n",
    "    # Process XML files\n",
    "    xml_results = xml_processor.process_all_files()\n",
    "    \n",
    "    # Match with Zotero data\n",
    "    matches = {}\n",
    "    for lib_name in ['portal', 'search']:\n",
    "        matches[lib_name] = {\n",
    "            'deduped': xml_processor.match_with_zotero(\n",
    "                xml_results[lib_name]['deduped'],\n",
    "                zotero_analyzer.libraries[lib_name]\n",
    "            ),\n",
    "            'duplicates': xml_processor.match_with_zotero(\n",
    "                xml_results[lib_name]['duplicates'],\n",
    "                zotero_analyzer.libraries[lib_name]\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    return matches, xml_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 12:29:55 - Starting analysis\n",
      "2024-12-04 12:29:55 - Initializing processors\n",
      "2024-12-04 12:29:55 - Processing XML files\n",
      "2024-12-04 12:29:55,615 - XMLProcessor - INFO - Processing portal library files\n",
      "2024-12-04 12:29:55 - Processing portal library files\n",
      "2024-12-04 12:29:55,615 - XMLProcessor - INFO - Processing deduped file for portal\n",
      "2024-12-04 12:29:55 - Processing deduped file for portal\n",
      "2024-12-04 12:29:55,616 - XMLProcessor - INFO - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55 - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55,628 - XMLProcessor - DEBUG - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55 - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55,629 - XMLProcessor - INFO - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55 - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55,629 - XMLProcessor - DEBUG - Columns found: []\n",
      "2024-12-04 12:29:55 - Columns found: []\n",
      "2024-12-04 12:29:55,631 - XMLProcessor - WARNING - No data processed for deduped file in portal\n",
      "2024-12-04 12:29:55 - No data processed for deduped file in portal\n",
      "2024-12-04 12:29:55,632 - XMLProcessor - INFO - Processing duplicates file for portal\n",
      "2024-12-04 12:29:55 - Processing duplicates file for portal\n",
      "2024-12-04 12:29:55,632 - XMLProcessor - INFO - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55 - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55,697 - XMLProcessor - DEBUG - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55 - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55,699 - XMLProcessor - INFO - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55 - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml\n",
      "2024-12-04 12:29:55,699 - XMLProcessor - DEBUG - Columns found: []\n",
      "2024-12-04 12:29:55 - Columns found: []\n",
      "2024-12-04 12:29:55,702 - XMLProcessor - WARNING - No data processed for duplicates file in portal\n",
      "2024-12-04 12:29:55 - No data processed for duplicates file in portal\n",
      "2024-12-04 12:29:55,703 - XMLProcessor - INFO - Processing search library files\n",
      "2024-12-04 12:29:55 - Processing search library files\n",
      "2024-12-04 12:29:55,703 - XMLProcessor - INFO - Processing deduped file for search\n",
      "2024-12-04 12:29:55 - Processing deduped file for search\n",
      "2024-12-04 12:29:55,704 - XMLProcessor - INFO - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55 - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55,704 - XMLProcessor - DEBUG - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55 - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55,705 - XMLProcessor - INFO - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55 - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55,705 - XMLProcessor - DEBUG - Columns found: []\n",
      "2024-12-04 12:29:55 - Columns found: []\n",
      "2024-12-04 12:29:55,705 - XMLProcessor - WARNING - No data processed for deduped file in search\n",
      "2024-12-04 12:29:55 - No data processed for deduped file in search\n",
      "2024-12-04 12:29:55,706 - XMLProcessor - INFO - Processing duplicates file for search\n",
      "2024-12-04 12:29:55 - Processing duplicates file for search\n",
      "2024-12-04 12:29:55,706 - XMLProcessor - INFO - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55 - Attempting to parse: /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55,728 - XMLProcessor - DEBUG - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55 - XML namespaces: No namespaces found\n",
      "2024-12-04 12:29:55,729 - XMLProcessor - INFO - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55 - Successfully parsed 0 items from /Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml\n",
      "2024-12-04 12:29:55,730 - XMLProcessor - DEBUG - Columns found: []\n",
      "2024-12-04 12:29:55 - Columns found: []\n",
      "2024-12-04 12:29:55,731 - XMLProcessor - WARNING - No data processed for duplicates file in search\n",
      "2024-12-04 12:29:55 - No data processed for duplicates file in search\n",
      "2024-12-04 12:29:55 - No data for portal deduped\n",
      "2024-12-04 12:29:55 - No data for portal duplicates\n",
      "2024-12-04 12:29:55 - No data for search deduped\n",
      "2024-12-04 12:29:55 - No data for search duplicates\n",
      "2024-12-04 12:29:55 - portal Zotero data summary:\n",
      "2024-12-04 12:29:55 - Shape: (9600, 10)\n",
      "2024-12-04 12:29:55 - Columns: ['key', 'title', 'itemType', 'creators', 'date', 'DOI', 'url', 'collections', 'tags', 'abstractNote']\n",
      "2024-12-04 12:29:55 - Sample data:\n",
      "        key                                              title  \\\n",
      "0  Q3VCXAZ4  Recent paleolimnology of three lakes in the Fr...   \n",
      "1  NJ6E75M3  Water Quality Trends in the Fraser River Basin...   \n",
      "2  8EUFYQNE  Picketts et al_2017_Climate change and resourc...   \n",
      "3  BK9QIEJG  Hewitt_2019_Exploring Indigenous-led Collabora...   \n",
      "4  8X2UYQED                        Haas et al_Marc and Arc.pdf   \n",
      "\n",
      "         itemType                                           creators  \\\n",
      "0  journalArticle  [{'firstName': 'Alexander P.', 'lastName': 'Wo...   \n",
      "1          report  [{'name': 'Robin Regnier', 'creatorType': 'aut...   \n",
      "2      attachment                                                 []   \n",
      "3      attachment                                                 []   \n",
      "4      attachment                                                 []   \n",
      "\n",
      "         date                        DOI  \\\n",
      "0  2008-07-01  10.1007/s10933-007-9161-7   \n",
      "1     1998-11                              \n",
      "2                                          \n",
      "3                                          \n",
      "4                                          \n",
      "\n",
      "                                                 url   collections  \\\n",
      "0          https://doi.org/10.1007/s10933-007-9161-7  ['WTI7SWR5']   \n",
      "1  https://waves-vagues.dfo-mpo.gc.ca/library-bib...  ['WTI7SWR5']   \n",
      "2  https://content.ebscohost.com/ContentServer.as...            []   \n",
      "3      https://core.ac.uk/download/pdf/275697076.pdf            []   \n",
      "4  https://darchive.mblwhoilibrary.org/bitstream/...            []   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [{'tag': '2.m Water'}, {'tag': '2.o Sockeye Sa...   \n",
      "1  [{'tag': '2.m Water'}, {'tag': '25.a Environme...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "\n",
      "                                        abstractNote  \n",
      "0  The use of paleolimnology to reconstruct the c...  \n",
      "1  Water quality data have been collected at nine...  \n",
      "2                                                     \n",
      "3                                                     \n",
      "4                                                     \n",
      "2024-12-04 12:29:55 - search Zotero data summary:\n",
      "2024-12-04 12:29:55 - Shape: (2700, 10)\n",
      "2024-12-04 12:29:55 - Columns: ['key', 'title', 'itemType', 'creators', 'date', 'DOI', 'url', 'collections', 'tags', 'abstractNote']\n",
      "2024-12-04 12:29:55 - Sample data:\n",
      "        key                                              title     itemType  \\\n",
      "0  KLU7WISF                       Unsettling a settler society  bookSection   \n",
      "1  6KJQ95WB  Products, features and target markets: An inqu...       thesis   \n",
      "2  CVZCY5PX  Linking school based monitoring to land and wa...       thesis   \n",
      "3  6L6KLQCM                                     Texto completo   attachment   \n",
      "4  9HMBL37J  \"It’s our law to protect the land and the peop...       thesis   \n",
      "\n",
      "                                            creators        date DOI  \\\n",
      "0  [{'firstName': 'Leonie', 'lastName': 'Sanderco...  2012-01-01       \n",
      "1  [{'firstName': 'Diana', 'lastName': 'Kutzner',...        2009       \n",
      "2  [{'firstName': 'Eleanor', 'lastName': 'Parker'...        2022       \n",
      "3                                                 []                   \n",
      "4  [{'firstName': 'Nadia', 'lastName': 'Nowak', '...        2022       \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://sapienza.pure.elsevier.com/it/publicat...   \n",
      "1     http://arcabc.ca/islandora/object/unbc%3A6911/   \n",
      "2  https://unbc.arcabc.ca/islandora/object/unbc%3...   \n",
      "3  https://unbc.arcabc.ca/islandora/object/unbc%3...   \n",
      "4  https://unbc.arcabc.ca/islandora/object/unbc%3...   \n",
      "\n",
      "                collections                                tags  \\\n",
      "0  ['EG5BYG2X', '5JRIR9ZQ']  [{'tag': 'Saturation search (2)'}]   \n",
      "1              ['4EFHUEJI']  [{'tag': 'Saturation search (2)'}]   \n",
      "2  ['4EFHUEJI', '5JRIR9ZQ']                                  []   \n",
      "3                        []                                  []   \n",
      "4              ['4EFHUEJI']                                  []   \n",
      "\n",
      "                                        abstractNote  \n",
      "0  Prologue Five years ago we heard three anecdot...  \n",
      "1  Many Aboriginal communities in British Columbi...  \n",
      "2                                                     \n",
      "3                                                     \n",
      "4                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Summary:\n",
      "\n",
      "Portal Library:\n",
      "  deduped: 0 entries\n",
      "  duplicates: 0 entries\n",
      "  Zotero data: 9600 entries\n",
      "\n",
      "Search Library:\n",
      "  deduped: 0 entries\n",
      "  duplicates: 0 entries\n",
      "  Zotero data: 2700 entries\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import ast\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class XMLProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the XML processor with logging\"\"\"\n",
    "        self.logger = logging.getLogger('XMLProcessor')\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Add console handler if not already present\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.DEBUG)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "            \n",
    "        self.xml_data = {\n",
    "            'portal': {\n",
    "                'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml',\n",
    "                'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml'\n",
    "            },\n",
    "            'search': {\n",
    "                'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml',\n",
    "                'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def parse_xml_file(self, file_path):\n",
    "        \"\"\"Parse XML file with detailed logging\"\"\"\n",
    "        self.logger.info(f\"Attempting to parse: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                self.logger.error(f\"File not found: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Log namespace information\n",
    "            self.logger.debug(f\"XML namespaces: {root.nsmap if hasattr(root, 'nsmap') else 'No namespaces found'}\")\n",
    "            \n",
    "            items = []\n",
    "            for entry in root.findall('.//entry'):\n",
    "                try:\n",
    "                    item = {}\n",
    "                    \n",
    "                    # Extract and log each field\n",
    "                    item['title'] = entry.findtext('title', '')\n",
    "                    item['id'] = entry.findtext('id', '')\n",
    "                    item['updated'] = entry.findtext('updated', '')\n",
    "                    \n",
    "                    # Extract authors\n",
    "                    authors = []\n",
    "                    for author in entry.findall('.//author'):\n",
    "                        authors.append(author.findtext('name', ''))\n",
    "                    item['authors'] = '; '.join(authors)\n",
    "                    \n",
    "                    # Look for DOI in multiple possible locations\n",
    "                    doi = None\n",
    "                    doi_paths = [\n",
    "                        \".//zapi:DOI\",\n",
    "                        \".//DOI\",\n",
    "                        \".//doi\",\n",
    "                        \".//identifier[@type='doi']\"\n",
    "                    ]\n",
    "                    \n",
    "                    for path in doi_paths:\n",
    "                        try:\n",
    "                            doi_elem = entry.find(path)\n",
    "                            if doi_elem is not None:\n",
    "                                doi = doi_elem.text\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    \n",
    "                    item['DOI'] = doi if doi else ''\n",
    "                    \n",
    "                    items.append(item)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing entry: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            df = pd.DataFrame(items)\n",
    "            self.logger.info(f\"Successfully parsed {len(df)} items from {file_path}\")\n",
    "            self.logger.debug(f\"Columns found: {df.columns.tolist()}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing {file_path}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def process_all_files(self):\n",
    "        \"\"\"Process all XML files with error handling\"\"\"\n",
    "        results = {}\n",
    "        for lib_name, paths in self.xml_data.items():\n",
    "            self.logger.info(f\"Processing {lib_name} library files\")\n",
    "            results[lib_name] = {}\n",
    "            \n",
    "            for data_type, file_path in paths.items():\n",
    "                self.logger.info(f\"Processing {data_type} file for {lib_name}\")\n",
    "                df = self.parse_xml_file(file_path)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    self.logger.info(f\"Successfully processed {data_type} file for {lib_name}\")\n",
    "                    self.logger.debug(f\"Sample data:\\n{df.head()}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No data processed for {data_type} file in {lib_name}\")\n",
    "                    \n",
    "                results[lib_name][data_type] = df\n",
    "                \n",
    "        return results\n",
    "\n",
    "def debug_analyze_xml_and_zotero():\n",
    "    \"\"\"Debug version with comprehensive logging\"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler(f'zotero_debug_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('ZoteroAnalysis')\n",
    "    \n",
    "    logger.info(\"Starting analysis\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize processors\n",
    "        logger.info(\"Initializing processors\")\n",
    "        xml_processor = XMLProcessor()\n",
    "        zotero_analyzer = ZoteroAnalyzer()\n",
    "        \n",
    "        # Process XML files\n",
    "        logger.info(\"Processing XML files\")\n",
    "        xml_results = xml_processor.process_all_files()\n",
    "        \n",
    "        # Debug output for XML results\n",
    "        for lib_name in ['portal', 'search']:\n",
    "            for data_type in ['deduped', 'duplicates']:\n",
    "                df = xml_results[lib_name][data_type]\n",
    "                if not df.empty:\n",
    "                    logger.info(f\"{lib_name} {data_type} data summary:\")\n",
    "                    logger.info(f\"Shape: {df.shape}\")\n",
    "                    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "                    logger.info(f\"Sample data:\\n{df.head()}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No data for {lib_name} {data_type}\")\n",
    "        \n",
    "        # Debug output for Zotero data\n",
    "        for lib_name, df in zotero_analyzer.libraries.items():\n",
    "            logger.info(f\"{lib_name} Zotero data summary:\")\n",
    "            logger.info(f\"Shape: {df.shape}\")\n",
    "            logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "            logger.info(f\"Sample data:\\n{df.head()}\")\n",
    "        \n",
    "        return xml_results, zotero_analyzer.libraries\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in analysis: {str(e)}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "# Run the debug analysis\n",
    "xml_results, zotero_data = debug_analyze_xml_and_zotero()\n",
    "\n",
    "# Print summary of results\n",
    "if xml_results and zotero_data:\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    for lib_name in ['portal', 'search']:\n",
    "        print(f\"\\n{lib_name.title()} Library:\")\n",
    "        for data_type in ['deduped', 'duplicates']:\n",
    "            if lib_name in xml_results and data_type in xml_results[lib_name]:\n",
    "                df = xml_results[lib_name][data_type]\n",
    "                print(f\"  {data_type}: {len(df)} entries\")\n",
    "            else:\n",
    "                print(f\"  {data_type}: No data\")\n",
    "        \n",
    "        if lib_name in zotero_data:\n",
    "            print(f\"  Zotero data: {len(zotero_data[lib_name])} entries\")\n",
    "        else:\n",
    "            print(\"  Zotero data: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PORTAL - DEDUPED:\n",
      "      database database_name   database_path source-app source-app_name  \\\n",
      "0  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "1  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "2  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "3  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "4  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "\n",
      "  source-app_version ref-type    ref-type_name authors keywords  \n",
      "0               16.0       17  Journal Article    None     None  \n",
      "1               16.0       17  Journal Article    None     None  \n",
      "2               16.0       17  Journal Article    None     None  \n",
      "3               16.0       17  Journal Article    None     None  \n",
      "4               16.0       17  Journal Article    None     None  \n",
      "\n",
      "PORTAL - DUPLICATES:\n",
      "      database database_name   database_path source-app source-app_name  \\\n",
      "0  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "1  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "2  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "3  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "4  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "\n",
      "  source-app_version ref-type    ref-type_name authors keywords  \n",
      "0               16.0       17  Journal Article    None     None  \n",
      "1               16.0       17  Journal Article    None     None  \n",
      "2               16.0       17  Journal Article    None     None  \n",
      "3               16.0       17  Journal Article    None     None  \n",
      "4               16.0       17  Journal Article    None     None  \n",
      "\n",
      "SEARCH - DEDUPED:\n",
      "      database database_name   database_path source-app source-app_name  \\\n",
      "0  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "\n",
      "  source-app_version ref-type    ref-type_name authors keywords  \n",
      "0               16.0       17  Journal Article    None     None  \n",
      "\n",
      "SEARCH - DUPLICATES:\n",
      "      database database_name   database_path source-app source-app_name  \\\n",
      "0  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "1  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "2  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "3  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "4  EndNote.enl   EndNote.enl  c:\\EndNote.enl    EndNote         EndNote   \n",
      "\n",
      "  source-app_version ref-type    ref-type_name authors keywords  \n",
      "0               16.0       17  Journal Article    None     None  \n",
      "1               16.0       17  Journal Article    None     None  \n",
      "2               16.0       17  Journal Article    None     None  \n",
      "3               16.0       17  Journal Article    None     None  \n",
      "4               16.0       17  Journal Article    None     None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_record(record):\n",
    "    record_dict = {}\n",
    "    \n",
    "    # Extract elements with possible attributes\n",
    "    elements_with_attrs = ['database', 'source-app', 'ref-type']\n",
    "    for elem_name in elements_with_attrs:\n",
    "        elem = record.find(elem_name)\n",
    "        if elem is not None:\n",
    "            # Text content\n",
    "            record_dict[elem_name] = elem.text.strip() if elem.text else None\n",
    "            # Attributes\n",
    "            for attr_name, attr_value in elem.attrib.items():\n",
    "                record_dict[f'{elem_name}_{attr_name}'] = attr_value.strip()\n",
    "    \n",
    "    # Extract simple elements\n",
    "    simple_elements = ['pages', 'volume', 'number', 'issue', 'isbn',\n",
    "                       'abstract', 'research-notes', 'language',\n",
    "                       'electronic-resource-num', 'access-date']\n",
    "    for elem_name in simple_elements:\n",
    "        elem = record.find(elem_name)\n",
    "        if elem is not None and elem.text:\n",
    "            record_dict[elem_name] = elem.text.strip()\n",
    "    \n",
    "    # Extract titles\n",
    "    titles_elem = record.find('titles')\n",
    "    if titles_elem is not None:\n",
    "        for title_type in ['title', 'secondary-title', 'short-title']:\n",
    "            title_elem = titles_elem.find(title_type)\n",
    "            if title_elem is not None and title_elem.text:\n",
    "                record_dict[title_type] = title_elem.text.strip()\n",
    "    \n",
    "    # Extract periodical information\n",
    "    periodical_elem = record.find('periodical')\n",
    "    if periodical_elem is not None:\n",
    "        for periodical_type in ['full-title', 'abbr-1']:\n",
    "            periodical_sub_elem = periodical_elem.find(periodical_type)\n",
    "            if periodical_sub_elem is not None and periodical_sub_elem.text:\n",
    "                record_dict[periodical_type] = periodical_sub_elem.text.strip()\n",
    "    \n",
    "    # Extract authors\n",
    "    authors = []\n",
    "    for author_elem in record.findall('contributors/authors/author'):\n",
    "        if author_elem.text:\n",
    "            authors.append(author_elem.text.strip())\n",
    "    record_dict['authors'] = '; '.join(authors) if authors else None\n",
    "    \n",
    "    # Extract keywords\n",
    "    keywords = []\n",
    "    for keyword_elem in record.findall('keywords/keyword'):\n",
    "        if keyword_elem.text:\n",
    "            keywords.append(keyword_elem.text.strip())\n",
    "    record_dict['keywords'] = '; '.join(keywords) if keywords else None\n",
    "    \n",
    "    # Extract dates\n",
    "    year_elem = record.find('dates/year')\n",
    "    if year_elem is not None and year_elem.text:\n",
    "        record_dict['year'] = year_elem.text.strip()\n",
    "    pub_date_elem = record.find('dates/pub-dates/date')\n",
    "    if pub_date_elem is not None and pub_date_elem.text:\n",
    "        record_dict['pub-date'] = pub_date_elem.text.strip()\n",
    "    \n",
    "    # Extract URL\n",
    "    url_elem = record.find('urls/web-urls/url')\n",
    "    if url_elem is not None and url_elem.text:\n",
    "        record_dict['url'] = url_elem.text.strip()\n",
    "    \n",
    "    return record_dict\n",
    "\n",
    "def parse_xml_to_dataframe(file_path):\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Failed to parse XML file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    records = []\n",
    "    for record in root.findall('.//record'):\n",
    "        record_dict = parse_record(record)\n",
    "        records.append(record_dict)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "# File paths dictionary\n",
    "xml_data = {\n",
    "    'portal': {\n",
    "        'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0909.xml',\n",
    "        'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Portal/Deduplicator/Untitled_deduplicated 2024-12-02_Time0909.xml'\n",
    "    },\n",
    "    'search': {\n",
    "        'deduped': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_removed_duplicates 2024-12-02_Time0911.xml',\n",
    "        'duplicates': '/Users/ahmadjalil/Desktop/Zotero Project/Nechako Saturation Search/Deduplicator/Untitled_deduplicated 2024-12-02_Time0911.xml'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Parse and display the head of each DataFrame\n",
    "for category, paths in xml_data.items():\n",
    "    for status, path in paths.items():\n",
    "        print(f\"\\n{category.upper()} - {status.upper()}:\")\n",
    "        try:\n",
    "            df = parse_xml_to_dataframe(path)\n",
    "            if not df.empty:\n",
    "                print(df.head())\n",
    "            else:\n",
    "                print(f\"No data found in {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
